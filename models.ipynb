{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbRRtUi8gusR"
      },
      "source": [
        "#Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udAVjUsxgpre"
      },
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn import svm\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "!pip install GmdhPy\n",
        "from gmdhpy.gmdh import MultilayerGMDH\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "!pip install SALib\n",
        "from SALib.sample import fast_sampler\n",
        "from SALib.analyze import fast"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L165yLO8iUWk"
      },
      "source": [
        "#preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYLUrECkg6LP"
      },
      "source": [
        "#importing data\n",
        "df = pd.read_csv('data.csv')\n",
        "df = df.sample(frac=1)\n",
        "inputs = df.iloc[:,:6]\n",
        "outputs = df.iloc[:,6:7]\n",
        "#scaling features\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(inputs)\n",
        "scaled_inputs = scaler.transform(inputs)\n",
        "#training/testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, outputs, \n",
        "                                                    shuffle=False,\n",
        "                                                    test_size=0.15)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRFx9KijNQGw"
      },
      "source": [
        "#SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrNEs17qNUB1"
      },
      "source": [
        "#hyperparameters_optimization\n",
        "kernels = ['rbf', 'poly', 'sigmoid']\n",
        "gammas = ['auto', 'scale']\n",
        "Cs = [1, 9]\n",
        "epsilons = [0.01, 0.1]\n",
        "val=np.empty((24,2))\n",
        "i=0\n",
        "x = 0.0\n",
        "for kernel in kernels:\n",
        "    for gamma in gammas:\n",
        "       for C in Cs:\n",
        "         for epsilon in epsilons:\n",
        "           start_time = time.time()\n",
        "           regr = svm.SVR(kernel=kernel, gamma=gamma, C=C, epsilon=epsilon)\n",
        "           cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "           n_scores = cross_val_score(regr, X_train, y_train, scoring='r2', cv=cv, n_jobs=-1, error_score='raise')\n",
        "           valdiation_r2 = mean(n_scores) / std(n_scores)\n",
        "           t = time.time()-start_time\n",
        "           print(valdiation_r2, t)\n",
        "           val[i:i+1,:] = [valdiation_r2, t]\n",
        "           i+=1\n",
        "           if valdiation_r2 > x:\n",
        "            x = valdiation_r2\n",
        "            a = kernel\n",
        "            b = gamma\n",
        "            c = C\n",
        "            d = epsilon\n",
        "np.savetxt('val.csv', val, delimiter =',')\n",
        "a, b, c, d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HJbf0ZOlPHA"
      },
      "source": [
        "#model_training\n",
        "t1 = time.time()\n",
        "regr = svm.SVR(kernel='rbf', gamma='auto', C=9, epsilon = 0.01)\n",
        "regr.fit(X_train, y_train)\n",
        "t = -t1 + time.time()\n",
        "#model_testing\n",
        "test_r2 = regr.score(X_test, y_test)\n",
        "test_mean_square_error = mean_squared_error(y_test, regr.predict(X_test))\n",
        "test_r2, test_mean_square_error, t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYTauhYSv_5X"
      },
      "source": [
        "#ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh_Zutlvs9lR"
      },
      "source": [
        "#hyperparameters_optimization\n",
        "hidden_layer_sizes = [(50,), (150,), (20,20)]\n",
        "activations = ['relu', 'logistic', 'tanh']\n",
        "alphas = [0.1, 1.0, 10.0]\n",
        "x = 0.0\n",
        "val=np.empty((27,2))\n",
        "i=0\n",
        "for hidden_layer_size in hidden_layer_sizes:\n",
        "  for activation in activations:\n",
        "      for alpha in alphas:\n",
        "        start_time = time.time()\n",
        "        ann = MLPRegressor(hidden_layer_sizes=hidden_layer_size,\n",
        "                   activation=activation,\n",
        "                   solver = 'lbfgs',\n",
        "                   alpha = alpha)\n",
        "        cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "        n_scores = cross_val_score(ann, X_train, y_train, scoring='r2', cv=cv, n_jobs=-1, error_score='raise')\n",
        "        #print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
        "        valdiation_r2 = mean(n_scores) / std(n_scores)\n",
        "        t = time.time()-start_time\n",
        "        val[i:i+1,:] = [valdiation_r2, t]\n",
        "        i+=1\n",
        "        print(valdiation_r2, t)\n",
        "\n",
        "        if valdiation_r2 > x:\n",
        "          x = valdiation_r2\n",
        "          a = hidden_layer_size\n",
        "          b = activation\n",
        "          c = alpha\n",
        "np.savetxt('val.csv', val, delimiter =',')\n",
        "a, b, c\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqDqjRdEbHyF"
      },
      "source": [
        "#model_training\n",
        "t1 = time.time()\n",
        "ann = MLPRegressor(hidden_layer_sizes=(50,),\n",
        "                   solver = 'lbfgs',\n",
        "                   max_iter=10000,\n",
        "                   early_stopping=True,\n",
        "                   alpha = 0.1\n",
        "ann.fit(X_train, y_train)\n",
        "t = -t1 + time.time()\n",
        "#model_testing\n",
        "test_r2 = ann.score(X_test, y_test)\n",
        "test_mean_square_error = mean_squared_error(y_test, ann.predict(X_test))\n",
        "test_r2, test_mean_square_error, t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SauSxXizMQk"
      },
      "source": [
        "#GMDH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDbhYoxCwyKG"
      },
      "source": [
        "#hyperparameters_optimization\n",
        "ref_functions = ['linear_cov', 'quadratic', 'cubic']\n",
        "criterion_types = ['test', 'bias']\n",
        "layer_err_criterions = ['avg', 'top']\n",
        "\n",
        "for ref_function in ref_functions:\n",
        "  for criterion_type in criterion_types:\n",
        "    for layer_err_criterion in layer_err_criterions:\n",
        "      gmdh = MultilayerGMDH(normalize=False,\n",
        "                            ref_functions=ref_function,\n",
        "                            criterion_type=criterion_type,\n",
        "                            layer_err_criterion=layer_err_criterion)\n",
        "\n",
        "      gmdh.fit(X_train, y_train.to_numpy())\n",
        "      training_r2 = r2_score(y_test, gmdh.predict(X_test))\n",
        "\n",
        "\n",
        "      if training_r2 > x:\n",
        "          x = training_r2\n",
        "          a = ref_function\n",
        "          b = criterion_type\n",
        "          c = layer_err_criterion\n",
        "a,b,c      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laC3z06a7GUL"
      },
      "source": [
        "#model_training\n",
        "t1 = time.time()\n",
        "gmdh = MultilayerGMDH(normalize=False,\n",
        "                      ref_functions=('quadratic','cubic', 'linear_cov'),\n",
        "                      criterion_type='test',\n",
        "                      layer_err_criterion='avg')\n",
        "gmdh.fit(X_train.to_numpy(), y_train.to_numpy())\n",
        "t = -t1 + time.time()\n",
        "#model_testing\n",
        "test_r2 = r2_score(y_test, gmdh.predict(X_test.to_numpy()))\n",
        "test_mean_square_error = mean_squared_error(y_test, gmdh.predict(X_test.to_numpy()))\n",
        "test_r2, test_mean_square_error, t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X14UdQjDoElJ"
      },
      "source": [
        "#XGBoost regression\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6AP9TYtn6IR"
      },
      "source": [
        "#hyperparameters_optimization\n",
        "losses = ['ls', 'lad']\n",
        "n_estimators = [100, 500]\n",
        "max_features = [2, 3]\n",
        "max_depths = [2, 3, 4]\n",
        "x=0.0\n",
        "val=np.empty((24,2))\n",
        "i=0\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "for loss in losses:\n",
        "  for n_estimator in n_estimators:\n",
        "    for max_feature in max_features:\n",
        "      for max_depth in max_depths:\n",
        "        t1 = time.time()\n",
        "        xgb = GradientBoostingRegressor(loss=loss,\n",
        "                                        n_estimators=n_estimator,\n",
        "                                        max_features=max_feature,\n",
        "                                        max_depth=max_depth)\n",
        "        n_scores = cross_val_score(xgb, X_train, y_train, scoring='r2', cv=cv, n_jobs=-1, error_score='raise')\n",
        "        #print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
        "        valdiation_r2 = mean(n_scores) / std(n_scores)\n",
        "        t = time.time() - t1\n",
        "        val[i:i+1,:] = [valdiation_r2, t]\n",
        "        i+=1\n",
        "        print(valdiation_r2,t)\n",
        "\n",
        "        if valdiation_r2 > x:\n",
        "          x = valdiation_r2\n",
        "          a = loss\n",
        "          b = n_estimator\n",
        "          c = max_feature\n",
        "          d = max_depth\n",
        "np.savetxt('val.csv', val, delimiter =',')\n",
        "a, b, c, d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QC1LYG2ybJM"
      },
      "source": [
        "#model_training\n",
        "t1 = time.time()\n",
        "xgb = GradientBoostingRegressor(loss='ls',\n",
        "                                n_estimators=500,\n",
        "                                max_features=3,\n",
        "                                subsample=1)\n",
        "xgb.fit(X_train, y_train)\n",
        "t= -t1 + time.time()\n",
        "#model_testing\n",
        "test_r2 = xgb.score(X_test, y_test)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "test_mean_square_error = mean_squared_error(y_test, xgb.predict(X_test))\n",
        "test_r2, test_mean_square_error, t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykt2QY9T-nfP"
      },
      "source": [
        "## Senstivity Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqDgTZQ3SCFC"
      },
      "source": [
        "problem = {'num_vars': 6,\n",
        "           'names': ['T', 'Bu', 'T_irr', 'D', 'T_ann', 'O/M'],\n",
        "           'bounds': [[300, 3200],\n",
        "                     [0, 100],\n",
        "                     [0, 3200],\n",
        "                      [80, 100],\n",
        "                      [300, 2000],\n",
        "                      [2.0, 2.1]]\n",
        "           }\n",
        "param_values = fast_sampler.sample(problem, 1000000)\n",
        "param_values = scaler.transform(param_values)\n",
        "Y = xgb.predict(param_values)\n",
        "Si = fast.analyze(problem, Y, print_to_console=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}